{
 "metadata": {
  "name": "",
  "signature": "sha256:829439c25c82e12d02df0d443d9ba447e0465ebded0ca5b03878b2c36c4fb68f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import nltk\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "\n",
      "class KaggleWord2VecUtility(object):\n",
      "    \"\"\"KaggleWord2VecUtility is a utility class for processing raw HTML text into segments for further learning\"\"\"\n",
      "\n",
      "    @staticmethod\n",
      "    def review_to_wordlist( review, remove_stopwords=False ):\n",
      "        # Function to convert a document to a sequence of words,\n",
      "        # optionally removing stop words.  Returns a list of words.\n",
      "        #\n",
      "        # 1. Remove HTML\n",
      "        review_text = BeautifulSoup(review).get_text()\n",
      "        #\n",
      "        # 2. Remove non-letters\n",
      "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "        #\n",
      "        # 3. Convert words to lower case and split them\n",
      "        words = review_text.lower().split()\n",
      "        #\n",
      "        # 4. Optionally remove stop words (false by default)\n",
      "        if remove_stopwords:\n",
      "            stops = set(stopwords.words(\"english\"))\n",
      "            words = [w for w in words if not w in stops]\n",
      "        #\n",
      "        # 5. Return a list of words\n",
      "        return(words)\n",
      "\n",
      "    # Define a function to split a review into parsed sentences\n",
      "    @staticmethod\n",
      "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "        # Function to split a review into parsed sentences. Returns a\n",
      "        # list of sentences, where each sentence is a list of words\n",
      "        #\n",
      "        # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "        raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
      "        #\n",
      "        # 2. Loop over each sentence\n",
      "        sentences = []\n",
      "        for raw_sentence in raw_sentences:\n",
      "            # If a sentence is empty, skip it\n",
      "            if len(raw_sentence) > 0:\n",
      "                # Otherwise, call review_to_wordlist to get a list of words\n",
      "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
      "                  remove_stopwords ))\n",
      "        #\n",
      "        # Return the list of sentences (each sentence is a list of words,\n",
      "        # so this returns a list of lists\n",
      "        return sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#  Author: Angela Chapman\n",
      "#  Date: 8/6/2014\n",
      "#\n",
      "#  This file contains code to accompany the Kaggle tutorial\n",
      "#  \"Deep learning goes to the movies\".  The code in this file\n",
      "#  is for Part 1 of the tutorial on Natural Language Processing.\n",
      "#\n",
      "# *************************************** #\n",
      "\n",
      "import os\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "#from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    train = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'labeledTrainData.tsv'), header=0, \\\n",
      "                    delimiter=\"\\t\", quoting=3)\n",
      "    test = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'testData.tsv'), header=0, delimiter=\"\\t\", \\\n",
      "                   quoting=3 )\n",
      "\n",
      "    print 'The first review is:'\n",
      "    print train[\"review\"][0]\n",
      "\n",
      "    raw_input(\"Press Enter to continue...\")\n",
      "\n",
      "\n",
      "    print 'Download text data sets. If you already have NLTK datasets downloaded, just close the Python download window...'\n",
      "    nltk.download()  # Download text data sets, including stop words\n",
      "\n",
      "    # Initialize an empty list to hold the clean reviews\n",
      "    clean_train_reviews = []\n",
      "\n",
      "    # Loop over each review; create an index i that goes from 0 to the length\n",
      "    # of the movie review list\n",
      "\n",
      "    print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
      "    for i in xrange( 0, len(train[\"review\"])):\n",
      "        clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train[\"review\"][i], True)))\n",
      "\n",
      "\n",
      "    # ****** Create a bag of words from the training set\n",
      "    #\n",
      "    print \"Creating the bag of words...\\n\"\n",
      "\n",
      "\n",
      "    # Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
      "    # bag of words tool.\n",
      "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
      "                             tokenizer = None,    \\\n",
      "                             preprocessor = None, \\\n",
      "                             stop_words = None,   \\\n",
      "                             max_features = 5000)\n",
      "\n",
      "    # fit_transform() does two functions: First, it fits the model\n",
      "    # and learns the vocabulary; second, it transforms our training data\n",
      "    # into feature vectors. The input to fit_transform should be a list of\n",
      "    # strings.\n",
      "    train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
      "\n",
      "    # Numpy arrays are easy to work with, so convert the result to an\n",
      "    # array\n",
      "    train_data_features = train_data_features.toarray()\n",
      "\n",
      "    # ******* Train a random forest using the bag of words\n",
      "    #\n",
      "    print \"Training the random forest (this may take a while)...\"\n",
      "\n",
      "\n",
      "    # Initialize a Random Forest classifier with 100 trees\n",
      "    forest = RandomForestClassifier(n_estimators = 30)\n",
      "\n",
      "    # Fit the forest to the training set, using the bag of words as\n",
      "    # features and the sentiment labels as the response variable\n",
      "    #\n",
      "    # This may take a few minutes to run\n",
      "    forest = forest.fit( train_data_features, train[\"sentiment\"] )\n",
      "\n",
      "\n",
      "\n",
      "    # Create an empty list and append the clean reviews one by one\n",
      "    clean_test_reviews = []\n",
      "\n",
      "    print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
      "    for i in xrange(0,len(test[\"review\"])):\n",
      "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test[\"review\"][i], True)))\n",
      "\n",
      "    # Get a bag of words for the test set, and convert to a numpy array\n",
      "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
      "    test_data_features = test_data_features.toarray()\n",
      "\n",
      "    # Use the random forest to make sentiment label predictions\n",
      "    print \"Predicting test labels...\\n\"\n",
      "    result = forest.predict(test_data_features)\n",
      "\n",
      "    # Copy the results to a pandas dataframe with an \"id\" column and\n",
      "    # a \"sentiment\" column\n",
      "    output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "    # Use pandas to write the comma-separated output file\n",
      "    output.to_csv(os.path.join(os.path.dirname(__file__), 'data', 'Bag_of_Words_model.csv'), index=False, quoting=3)\n",
      "    print \"Wrote results to Bag_of_Words_model.csv\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name '__file__' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-61a69a915bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labeledTrainData.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'testData.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m                    \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}